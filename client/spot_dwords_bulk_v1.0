#!/usr/bin/env python

import os
import sys
import nltk
import re
from nltk import word_tokenize
from nltk.util import ngrams
from nltk.collocations import *
from os.path import expanduser, exists
from collections import Counter
import json
from MongoCorpus import MongoDBCorpusReader

global config

#Private definitions begin

def read_history(fileName):
   result = ''
   filePath = "%s/%s" % (history_path, fileName)
   try:
       if os.path.exists(filePath):
          f = open(filePath,"r")
          result = f.read()
   except Exception as inst:
       print "Couldn't read the grammar file (%s)" % (filePath);
       print inst
       stop(-1)
   return result

def read_config(fileName):
   result = ''
   filePath = "%s/%s" % (config_path, fileName)
   try:
       if os.path.exists(filePath):
          with open(filePath) as data:
             result = json.load(data)
   except Exception as inst:
       print "Couldn't read the config file (%s)" % (filePath);
       print inst
       stop(-1)
   return result

def tokenize(input_file, encoding):
    lst =[]
    file_path = "%s/%s" % (history_path, input_file)
    #wnl = nltk.stem.WordNetLemmatizer()
    #tokens = [wnl.lemmatize(token) for token in nltk.word_tokenize("%s" % input)]
    with open(file_path, 'r') as f:
        for sent in f:
            sent = sent.lower()
            sent = re.findall('\w+', sent)
            for word in sent:
                lst.append(word)
    return lst

def stop(status):
   exit(status)

#private definitions end


home = expanduser("~")
nltk.data.path.append(home + '/nltk_data/')
app_path = "%s/nltk-api" % (home)
client_path = "%s/client" % (app_path)
grammar_path = "%s/grammar" % (client_path)
history_path = "%s/history" % (client_path)
config_path = "%s/config" % (client_path)

#read config
config = read_config('settings.json')

fileName = sys.argv[1]
print "Input: %s" % (fileName)
tokens = tokenize(fileName, 'utf-8')

stop(-1)
unigram_measures = ngrams(tokens, 1)
bigram_measures = nltk.collocations.BigramAssocMeasures()
trigram_measures = nltk.collocations.TrigramAssocMeasures()

bi_finder = BigramCollocationFinder.from_words(tokens)
tri_finder = TrigramCollocationFinder.from_words(tokens)

bi_finder.apply_freq_filter(2)
tri_finder.apply_freq_filter(2)

uni_output = nltk.FreqDist(unigram_measures)
for k,v in uni_output.items():
    print "{0} {1} = {2}".format(k[0], v, nltk.pos_tag(k))

print bi_finder.nbest(bigram_measures.likelihood_ratio, 10)
bi_output = bi_finder.ngram_fd.viewitems()
for i, j in bi_output:
  print("{0} {1} {2}".format(i[0], i[1], j))

print tri_finder.nbest(trigram_measures.likelihood_ratio, 10)
tri_output = tri_finder.ngram_fd.viewitems()
for i, j in tri_output:
  print("{0} {1} {2} {3}".format(i[0], i[1], i[2], j))
  
stop(0)
