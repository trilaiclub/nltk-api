#!/usr/bin/env python

import os
import sys
import nltk
from os.path import expanduser, exists
from nltk.tokenize import RegexpTokenizer
from nltk.chunk.regexp import RegexpParser, RegexpChunkParser
from nltk.tree import Tree
from nltk import draw
import json

global t
global next_input

#Private definitions begin

def pre_process(text):
   tokenizer = RegexpTokenizer(r'(\w+|\d+)')
   tokens = tokenizer.tokenize(text)
   return tokens

def apply_grammar(text):
  tagged = nltk.pos_tag(text.split(' '))
  grammar = read_grammar('grammar.cfg')
  #print grammar
  parser = RegexpParser(grammar)
  #print parser
  chunked_text = parser.parse(tagged)
  return plant_a_tree(chunked_text)

def read_grammar(fileName):
   result = ''
   filePath = "%s/%s" % (grammar_path, fileName)
   try:
       if os.path.exists(filePath):
          f = open(filePath,"r")
          result = f.read()
   except Exception as inst:
       print "Couldn't read the grammar file (%s)" % (filePath);
       print inst
       stop(-1)
   return result

def stop(status):
   exit(status)


def extract_tag(psent):
  for subtree in psent.subtrees():
    if subtree.label() == tag:
      yield ' '.join(word for word,tag in subtree.leaves())

def plant_a_tree(text):
   return Tree.fromstring(str(text), read_leaf=lambda x: x.split("/"))

def run(ninput):
   if len(ninput) > 0:
      tree = apply_grammar(ninput)
   else:
      tree = Tree.fromstring(str("(S )"))
   return tree   

def replace_word(text, next_value):
   next_value = next_value.replace(text, "", 1)
   next_value = next_value.strip()
   return next_value

def lemmatizer(phrase):
   result = ''
   wnl = nltk.stem.WordNetLemmatizer()
   wtext = nltk.word_tokenize("%s" % phrase)
   wtokens = nltk.pos_tag(wtext)
   for wtoken,pos in wtokens:
      result += wnl.lemmatize(wtoken) + ' '
   return result[:-1]
#private definitions end


home = expanduser("~")
nltk.data.path.append(home + '/nltk_data/')
app_path = "%s/nltk-api" % (home)
client_path = "%s/client" % (app_path)
grammar_path = "%s/grammar" % (client_path)

input = ' '.join(sys.argv[1:])
tokens = nltk.word_tokenize("%s" % input)
tokens = filter(lambda word: word not in ',-!@#$%^&*()_+={}[]:;"\'<>.?/~`|\\', tokens)
#print tokens
input = ' '.join(tokens)
#print input
output = ''
tag = "SBG"


t = apply_grammar(input)
#print t
next_input = input

while len(t) > 0:
   op_tree = t[0] 
   #print op_tree
   try:
      for word in extract_tag(op_tree):
         #print "----O%s-" % word
         next_input = replace_word(word, next_input)
         word = lemmatizer(word)
         #print "lemmatize(" + word
         output += word +  ","
         #print "----O" + next_input
         t = run(next_input)
         #print "++++"
         #print t
         #print "==================================================="
   except AttributeError:
      #print t[0][0]
      next_input = replace_word(t[0][0], next_input)
      t = run(next_input)
   except Exception as inst:
       print inst
       stop(-1)


print output[:-1]

